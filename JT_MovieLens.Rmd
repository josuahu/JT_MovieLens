---
title: "Capstone MovieLens"
author: "Jozsef Toth"
date: "2024-05-07"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r prep, include=FALSE}
if(!require(tidyverse)) install.packages("tidyverse")
if(!require(caret)) install.packages("caret")

library(tidyverse)
library(caret)
library(gridExtra)

##########################################################
# Create edx and final_holdout_test sets 
##########################################################

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

options(timeout = 240)

dl <- "ml-10M100K.zip"
if(!file.exists(dl))
  download.file("https://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings_file <- "ml-10M100K/ratings.dat"
if(!file.exists(ratings_file))
  unzip(dl, ratings_file)

movies_file <- "ml-10M100K/movies.dat"
if(!file.exists(movies_file))
  unzip(dl, movies_file)

ratings <- as.data.frame(str_split(read_lines(ratings_file), fixed("::"), simplify = TRUE),
                         stringsAsFactors = FALSE)
colnames(ratings) <- c("userId", "movieId", "rating", "timestamp")
ratings <- ratings %>%
  mutate(userId = as.integer(userId),
         movieId = as.integer(movieId),
         rating = as.numeric(rating),
         timestamp = as.integer(timestamp))

movies <- as.data.frame(str_split(read_lines(movies_file), fixed("::"), simplify = TRUE),
                        stringsAsFactors = FALSE)
colnames(movies) <- c("movieId", "title", "genres")
movies <- movies %>%
  mutate(movieId = as.integer(movieId))

movielens <- left_join(ratings, movies, by = "movieId")

# Final hold-out test set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.6 or later
# set.seed(1) # if using R 3.5 or earlier
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in final hold-out test set are also in edx set
final_holdout_test <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from final hold-out test set back into edx set
removed <- anti_join(temp, final_holdout_test)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)

```

## Introduction

The objective of this project is to create a movie recommendation system based on the MovieLens dataset.
The input dataset (edx) contains around 9 million rows (ratings), but the real life dataset can be much larger which makes it challenging to apply a fast and complex machine learning approach. Before model development, the input data set is divided into 2 parts: training and testing data in the ratio of 90% vs 10%.
Generally, I will follow the methodology in the book and first calculate the effect of the various properties (movie, user, genres) and then perform a regularization to penalize observations that have very few observations. At the end I will cut the prediction interval to fit better for the rating distribution.
The evaluation of the developed models will be done with RMSE (Root Mean Square Error).
The validation dataset (final_holdout_test) contains around 1 million ratings and will be only used for the final evaluation of the model.
This is my last course for HarvardX’s Data Science Professional Certificate program, and I hope I can apply what I’ve learned in the series.

Original dataset : https://grouplens.org/datasets/movielens/10m/

## Methods and model development

Before we go on let me to introduce two helper functions to evaluate our model and than create the training and test sets. The Evaluate_Model function will be used to calculate RMSE and accuracy. Accuracy is not in the focus of our model development, but still useful to have it. Accuracy will be calculated for half rounded numbers (0,0.5,1,1.5,etc) what are generated by the Round_Rating function.

```{r helper}
#Helper function to calculate RMSE and accuracy
#For accuracy we use the half rounded numbers (0,0.5,1,1.5,2,2.5, etc)
#For RMSE we use the caret function caret::RMSE
Evaluate_Model <- function(y_hat,model_name="Undefined",y=test_set$rating) {
  accuracy <- mean(Round_Rating(y_hat) == y)
  rmse <- RMSE(y_hat,y)
  data.frame(model_name,rmse,accuracy)
}

#Helper function to half round number to closest possible rating
#Possible rating values : (0,0.5,1,1.5,2,2.5,3,3.5,4,4.5,5)
Round_Rating <- function(y) {
  y_rounded <- round(y*2)/2
  y_rounded[y_rounded<0] <- 0
  y_rounded[y_rounded>5] <- 5
  y_rounded
}
```

The edx data set contains around 9 million rows. We divide this dataset into 2 parts: training and testing datasets with the ratio of 90 percent vs 10 percent, by using the following code:

```{r data partition}
#Create train and test sets
set.seed(1)
test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.1, list = FALSE)
train_set <- edx[-test_index,]
test_set <- edx[test_index,]
```

We will use the training dataset (rownum=`r nrow(train_set)`) to explore the general statistics of the variables and develop our model. The test dataset (rownum=`r nrow(test_set)`) will be used for evaluating our actual model. 
We have the following columns:

```{r, echo=FALSE}
colnames(test_set)
```

Rating column is our target what we want to predict. Title is not really useful for the prediction because it is different for all movies. Timestamp is an interesting variable, but not easy to include as a predictor. So we have three column what can be easily used as a predictor for rating : \
"userId"  - Different users like different movies \
"movieId" - Some movies are more popular than others \
"genres"  - The genre of the movie can affect the rating of the movie \

If we look at the distribution of the ratings, we will see some evidence:

```{r, echo=FALSE, fig.align = 'center', fig.height=3,fig.width=4}
mean_rating <- mean(test_set$rating)
test_set %>%
  ggplot(aes(rating)) +
  geom_histogram(binwidth = 0.5, fill = "cyan", col = "black") +
  xlab("rating")+ylab("number of ratings")+
  geom_vline(xintercept = mean_rating,color="red")+
  scale_x_continuous(breaks = c(0,0.5,1,1.5,2,2.5,3,3.5,4,4.5,5))+
  ggtitle("Ratings historgram")
```
  
```{r, echo=FALSE}
general_stats <- test_set %>%
  summarise(Description="Ratings in general",min=min(rating),max=max(rating),
            mean=round(mean(rating),2),median=median(rating))
knitr::kable(general_stats, layout="l-body-outset")
```

The rating range is from 0.5 to 5. The median rating has the most rates, the average rating is in the center of the distribution. An interesting finding is that users are more likely to give whole numbers (1, 2, etc.) than half-round numbers (0.5, 1.5, etc.). Now explore movie, user and genre ratings in details:

\newpage
```{r, echo=FALSE}
#Rating stats for movies
movie_stats <- test_set %>%
  group_by(movieId) %>%
  summarize(rating_count=n()) %>%
  summarise(Description="Rating counts for movies",
            min=min(rating_count),max=max(rating_count),
            mean=round(mean(rating_count),2),median=median(rating_count))

#Rating counts for movies
rating_chart_1 <- test_set %>%
  group_by(movieId) %>%
  summarize(avg_score=mean(rating)) %>%
  ggplot(aes(avg_score)) +
  geom_histogram(binwidth = 0.1, fill = "cyan", col = "black") +
  xlab("average rating")+ylab("number of movies")+
  ggtitle("Rating distribution for movies")

rating_chart_2 <- test_set %>%
  group_by(movieId) %>%
  summarize(rating_count=n()) %>%
  arrange(desc(rating_count)) %>%
  mutate(nr=row_number()) %>%
  ggplot(aes(nr,rating_count)) +
  geom_bar(stat="identity", col = "black") +
  geom_hline(aes(yintercept=as.numeric(movie_stats[1,4]),color="red")) +
  guides(color = "none") +
  xlab("movies")+ylab("number of ratings")+
  theme(axis.text.x = element_blank()) +
  ggtitle("Rating counts for movies")
```  
```{r, echo=FALSE, fig.align = 'left', fig.height=2.8}
grid.arrange(rating_chart_1,rating_chart_2,nrow=1)
```
```{r, echo=FALSE}
knitr::kable(movie_stats, layout="l-body-outset")
``` 
The average ratings shows similar distribution like before. We can see that there is a huge difference between the rating counts of the movies. There are movies that have only a few ratings and others that have over 1000. We know this is normal because famous movies can have more reviewers than others.

```{r, echo=FALSE}
#Rating stats for users
user_stats <- test_set %>%
  group_by(userId) %>%
  summarize(rating_count=n()) %>%
  summarise(Description="Rating counts for users",
            min=min(rating_count),max=max(rating_count),
            mean=round(mean(rating_count),2),median=median(rating_count))

#Rating counts for users
user_chart_1 <- test_set %>%
  group_by(userId) %>%
  summarize(avg_score=mean(rating)) %>%
  ggplot(aes(avg_score)) +
  geom_histogram(binwidth = 0.1, fill = "cyan", col = "black") +
  xlab("average rating")+ylab("number of users")+
  ggtitle("Ratings distribution for users")

user_chart_2 <- test_set %>%
  group_by(userId) %>%
  summarize(rating_count=n()) %>%
  arrange(desc(rating_count)) %>%
  mutate(nr=row_number()) %>%
  ggplot(aes(nr,rating_count)) +
  geom_bar(stat="identity", col = "black") +
  geom_hline(aes(yintercept=as.numeric(user_stats[1,4]),color="red")) +
  guides(color = "none") +
  theme(axis.text.x = element_blank()) +
  xlab("users")+ylab("number of ratings")+
  ggtitle("Rating counts for users")
```
```{r, echo=FALSE, fig.align = 'left', fig.height=2.8}
grid.arrange(user_chart_1,user_chart_2,nrow=1)
```
```{r, echo=FALSE}
knitr::kable(user_stats, layout="l-body-outset")
``` 
We can see very similar charts for users. There are very active users with more than 600 reviews and some only with a few.

\newpage
```{r, echo=FALSE}
#Rating counts for genres
genres_stats <- test_set %>%
  group_by(genres) %>%
  summarize(rating_count=n()) %>%
  summarise(Description="Rating counts for genres",
            min=min(rating_count),max=max(rating_count),
            mean=mean(rating_count),median=median(rating_count))

#Rating counts for genres
genres_chart_1 <- test_set %>%
  group_by(genres) %>%
  summarize(avg_score=mean(rating)) %>%
  ggplot(aes(avg_score)) +
  geom_histogram(binwidth = 0.1, fill = "cyan", col = "black") +
  xlab("average rating")+ylab("number of genres")+
  ggtitle("Ratings distribution for genres")

#Rating distribution for genres histogram
genres_chart_2 <- test_set %>%
  group_by(genres) %>%
  summarize(rating_count=n()) %>%
  arrange(desc(rating_count)) %>%
  mutate(nr=row_number()) %>%
  ggplot(aes(nr,rating_count)) +
  geom_bar(stat="identity", col = "black") +
  geom_hline(aes(yintercept=as.numeric(genres_stats[1,4]),color="red")) +
  guides(color = "none") +
  theme(axis.text.x = element_blank()) +
  xlab("genres")+ylab("number of ratings") +
  ggtitle("Rating counts for genres")
``` 

```{r, echo=FALSE, fig.align = 'left', fig.height=2.8}
grid.arrange(genres_chart_1,genres_chart_2,nrow=1)
```

```{r, echo=FALSE}
knitr::kable(genres_stats, layout="l-body-outset")
``` 

Genres show approximately the same distribution. Users like different movies, for example I like adventure and sci-fi movies. Users can give better ratings for the types of movies they like.

\
**Preparation of the model development**

First, let's examine the difference between choosing the median (`r general_stats[1,5]`) or the average rating (`r general_stats[1,4]`) as our predictor.
\

**Model 1 : Median rating alone**

```{r, echo=TRUE}
median_rating <- median(train_set$rating)
m1_eval <- Evaluate_Model(median_rating,"median alone")
m1_eval
```

**Model 2 : Mean rating alone**

```{r, echo=TRUE}
mean_rating = mean(train_set$rating)
m2_eval <- Evaluate_Model(mean_rating,"mean alone")
m2_eval
```

Median rating is more accurate, what is not a surprise, because it is an integer matching the most common rating in the test set. Mean is a better choice for RMSE optimization what is our main objective. We will continue the model development with checking the effect of the three selected variable (userId,movieId,genres).

\newpage
**Model 3 : Mean + User effect**

```{r, echo=TRUE}
user_effect <- train_set %>%
  group_by(userId) %>%
  summarize(u_e = mean(rating-mean_rating),u_n=n())

predicted_ratings <- test_set %>%
  left_join(user_effect,by=c("userId")) %>%
  rowwise() %>%
  mutate(y=sum(mean_rating,u_e,na.rm = TRUE)) %>%
  pull(y)
m3_eval <- Evaluate_Model(predicted_ratings,"mean+user effect")
m3_eval
```

**Model 4 : Mean + Movie effect**

```{r, echo=TRUE}
movie_effect <- train_set %>%
  group_by(movieId) %>%
  summarize(m_e = mean(rating-mean_rating),m_n=n())

predicted_ratings <- test_set %>%
  left_join(movie_effect,by=c("movieId")) %>%
  rowwise() %>%
  mutate(y=sum(mean_rating,m_e,na.rm = TRUE)) %>%
  pull(y)
m4_eval <- Evaluate_Model(predicted_ratings,"mean+movie effect")
m4_eval
```

**Model 5 : Mean + genres effect**

```{r, echo=TRUE}
genre_effect <- train_set %>%
  group_by(genres) %>%
  summarize(g_e = mean(rating-mean_rating),g_n=n())

predicted_ratings <- test_set %>%
  left_join(genre_effect,by=c("genres")) %>%
  rowwise() %>%
  mutate(y=sum(mean_rating,g_e,na.rm = TRUE)) %>%
  pull(y)
m5_eval <- Evaluate_Model(predicted_ratings,"mean+genres effect")
m5_eval
```

As we see the different models has different effect to RMSE. The order of possible effects will be chosen by RMSE decrease (larger decrease used first), so we will combine the models in the order of : movie, user and genres.

\
**Model 6 : Mean rating + movie effect + user effect**

```{r, echo=TRUE}
#Movie effect:
movie_effect <- movie_effect <- train_set %>%
  group_by(movieId) %>%
  summarize(m_e = mean(rating-mean_rating),m_n=n())

#User effect:
user_effect <- train_set %>%
  left_join(movie_effect,by=c("movieId")) %>%
  group_by(userId) %>%
  summarize(u_e = mean(rating-mean_rating-m_e),u_n=n())

#Predict (mean+movie+user effect):
predicted_ratings <- test_set %>%
  left_join(movie_effect,by=c("movieId")) %>%
  left_join(user_effect,by=c("userId")) %>%
  rowwise() %>%
  mutate(y=sum(mean_rating,m_e,u_e,na.rm = TRUE)) %>%
  pull(y)
m6_eval <- Evaluate_Model(predicted_ratings,"mean+movie+user effect")
m6_eval
```

**Model 7 : Mean rating + movie effect + user effect + genres effect**

```{r, echo=TRUE}
#Movie effect and user effect is the same as before...

#Calculate genre effect:
genre_effect <- train_set %>%
  left_join(movie_effect,by=c("movieId")) %>%
  left_join(user_effect,by=c("userId")) %>%
  group_by(genres) %>%
  summarize(g_e = mean(rating-mean_rating-m_e-u_e),g_n=n())

#Predict (mean+movie+user+genres effect):
predicted_ratings <- test_set %>%
  left_join(movie_effect,by=c("movieId")) %>%
  left_join(user_effect,by=c("userId")) %>%
  left_join(genre_effect,by=c("genres")) %>%
  rowwise() %>%
  mutate(y=sum(mean_rating,m_e,u_e,g_e,na.rm = TRUE)) %>%
  pull(y)
m7_eval <- Evaluate_Model(predicted_ratings,"mean+movie+user+genres effect")
m7_eval
```

We have used the three selected variables (movieId,userId,genres) to optimize our model, now it's time to fine-tune our algorithm. As we saw earlier, there are movies/users/genres that have few ratings. Observations with a small number of ratings are not really reliable, so we introduce a lambda penalty value to reduce these effects. Please check Model 8 for the calculation details, where we simple replace the mean function to a manual calculation where the row number is increased by lambda. 

```{r, echo=FALSE}
#Function to calculate RMSE for a set of lambdas
Calculate_RMSEs_for_Lambda <- function(l) {
  rmses_lambda <- sapply(l, function(l){
    mean_rating = mean(train_set$rating)
    movie_effect <- train_set %>%
      group_by(movieId) %>%
      summarize(m_e = sum(rating-mean_rating)/(n()+l))
    user_effect <- train_set %>%
      left_join(movie_effect,by=c("movieId")) %>%
      group_by(userId) %>%
      summarize(u_e = sum(rating-mean_rating-m_e)/(n()+l))
    genre_effect <- train_set %>%
      left_join(movie_effect,by=c("movieId")) %>%
      left_join(user_effect,by=c("userId")) %>%
      group_by(genres) %>%
      summarize(g_e = sum(rating-mean_rating-m_e-u_e)/(n()+l))
    predicted_ratings <- test_set %>%
      left_join(movie_effect,by=c("movieId")) %>%
      left_join(user_effect,by=c("userId")) %>%
      left_join(genre_effect,by=c("genres")) %>%
      rowwise() %>%
      mutate(y=sum(mean_rating,m_e,u_e,g_e,na.rm = TRUE)) %>%
      pull(y)
    return(RMSE(predicted_ratings, test_set$rating))
  })
}
```

Lets try lambdas from 0 to 10 and than refine it between 4.7 and 4.9 :

```{r, echo=FALSE, fig.height=3}
#Try lambdas from 1 to 10
lambdas_1 <- seq(0:10)
rmse_list_1 <- Calculate_RMSEs_for_Lambda(lambdas_1)
lambda_plot_1 <- ggplot(aes(lambdas_1,rmse_list_1),
                        data=data.frame(lambdas_1,rmse_list_1)) + 
                 geom_point() +
                 scale_x_continuous(breaks = 1:10) +
                 ylab("RMSE") + xlab("Lambda")

lambdas_2 <- seq(4.7,4.9,by=0.025)
rmse_list_2 <- Calculate_RMSEs_for_Lambda(lambdas_2)
lambda_plot_2 <- ggplot(aes(lambdas_2,rmse_list_2),
                        data=data.frame(lambdas_2,rmse_list_2)) + 
                 geom_point() +
                 ylab("RMSE") + xlab("Lambda")

grid.arrange(lambda_plot_1,lambda_plot_2,nrow=1)
```

We can choose the optimal value based on the charts :

Lambda = 4.755
\

**Model 8 with Lambda:**

```{r, echo=TRUE}
# Model with the selected lamba penality (4.75)
lambda <- 4.755
mean_rating = mean(train_set$rating)
movie_effect <- train_set %>%
  group_by(movieId) %>%
  summarize(m_e = sum(rating-mean_rating)/(n()+lambda))
user_effect <- train_set %>%
  left_join(movie_effect,by=c("movieId")) %>%
  group_by(userId) %>%
  summarize(u_e = sum(rating-mean_rating-m_e)/(n()+lambda))
genre_effect <- train_set %>%
  left_join(movie_effect,by=c("movieId")) %>%
  left_join(user_effect,by=c("userId")) %>%
  group_by(genres) %>%
  summarize(g_e = sum(rating-mean_rating-m_e-u_e)/(n()+lambda))
predicted_ratings <- test_set %>%
  left_join(movie_effect,by=c("movieId")) %>%
  left_join(user_effect,by=c("userId")) %>%
  left_join(genre_effect,by=c("genres")) %>%
  rowwise() %>%
  mutate(y=sum(mean_rating,m_e,u_e,g_e,na.rm = TRUE)) %>%
  pull(y)
m8_eval <- Evaluate_Model(predicted_ratings,"mean+movie+user+genres+lambda")
m8_eval
```

If we check the prediction range of our Model 8, we can see that the addition of different effects could produce too low and too high values as well :

```{r, echo=FALSE}
data.frame(Description="Rating range after prediction",
           min=min(predicted_ratings),max=max(predicted_ratings),
           mean=round(mean(predicted_ratings),2),median=median(predicted_ratings))
```

Based on the knowledge that the test set has only ratings between 0.5 and 5, we can simple cut the results in the final algorithm to fit this range.

## Results , the final model

**Model 9 : Final model with lambda and [0.5,5] prediction interval**

```{r, echo=TRUE}
# Final model
lambda <- 4.755
mean_rating = mean(train_set$rating)
movie_effect <- train_set %>%
  group_by(movieId) %>%
  summarize(m_e = sum(rating-mean_rating)/(n()+lambda))
user_effect <- train_set %>%
  left_join(movie_effect,by=c("movieId")) %>%
  group_by(userId) %>%
  summarize(u_e = sum(rating-mean_rating-m_e)/(n()+lambda))
genre_effect <- train_set %>%
  left_join(movie_effect,by=c("movieId")) %>%
  left_join(user_effect,by=c("userId")) %>%
  group_by(genres) %>%
  summarize(g_e = sum(rating-mean_rating-m_e-u_e)/(n()+lambda))
predicted_ratings <- test_set %>%
  left_join(movie_effect,by=c("movieId")) %>%
  left_join(user_effect,by=c("userId")) %>%
  left_join(genre_effect,by=c("genres")) %>%
  rowwise() %>%
  mutate(y=sum(mean_rating,m_e,u_e,g_e,na.rm = TRUE)) %>%
  mutate(y=ifelse(y>5,5,y),y=ifelse(y<0.5,0.5,y)) %>%
  pull(y)
m9_eval <- Evaluate_Model(predicted_ratings,"mean+movie+user+genres+lambda+interval")
m9_eval
```

\newpage

**Final model performance on the validation set (final_holdout_test) :**

```{r, echo=TRUE}
predicted_ratings <- final_holdout_test %>%
  left_join(movie_effect,by=c("movieId")) %>%
  left_join(user_effect,by=c("userId")) %>%
  left_join(genre_effect,by=c("genres")) %>%
  rowwise() %>%
  mutate(y=sum(mean_rating,m_e,u_e,g_e,na.rm = TRUE)) %>%
  mutate(y=ifelse(y>5,5,y),y=ifelse(y<0.5,0.5,y)) %>%
  pull(y)
final_eval <- Evaluate_Model(predicted_ratings,"Final holdout test",y = final_holdout_test$rating)
final_eval
```

## Conclusion

We have successfully developed a model with the following steps :

```{r, echo=FALSE}
summary <- bind_rows(m1_eval,m2_eval,m3_eval,m4_eval,m5_eval,
          m6_eval,m7_eval,m8_eval,m9_eval,final_eval)
knitr::kable(summary, layout="l-body-outset")
```

Some of our steps decreased RMSE significantly and others only slightly. The limitation of this work is that we have focused on creating a fast solution which needs only couple of minutes to run on an average computer. 

I can imagine the following improvements :\
- Split genres into individuals like (Action, Romance,etc...) and calculate effects for each.\
- As 'Ratings in general' histogram shows users are more likely to give integer values. We can use this information to refine the model.\
- Clustering users and movies by similarities can also be an option because this way we minimize the missing values in the User/Movie rating matrix\


Thanks you for taking the time to read my report.

